{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-93ce763a6d93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "from gensim.models import CoherenceModel\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "'''\n",
    "抓取tweets：包括company_twitter_name list中的22家cultured meat的company，抓取過程中通過include_rts=False過濾掉retweets，\n",
    "抓取後通過tweet.in_reply_to_status_id == None過濾掉所有reply\n",
    "'''\n",
    "consumer_key = 'YRWB6yizS63ZJUW2CpEUqS7TN'\n",
    "consumer_secret = 'ZdMYuSvrCPq2qv5yQ01wpE6FgaQJwvytuYuNnkuWdoMLpjO0sg'\n",
    "access_token = '1315688543784562688-UYAqRnekf0mDKoQgsZWoKiBGqlffNw'\n",
    "access_token_secret = 'wvORx0dHxp9uGXXjAjZEJ1hj8eLGcckRlOlmzvxhSqAEA'\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "tweets = []\n",
    "company_twitter_name = ['MemphisMeats','mosa_meat','_superMeat_','ModernMeadow',\n",
    "                        'MissionBarns','BalleticFoods','CubiqF',\n",
    "                        'NewAgeMeats','FinlessFoods','wildtypefoods','itsmeatable',\n",
    "                        'eatGOURMEY','eatjust','shiokmeats','makingmeat','BlueNaluInc','FutureMeat1','AvantMeats',\n",
    "                        'AlephFarms','biftekco','vowfood','LabFarmFoods']\n",
    "for company_name in company_twitter_name:\n",
    "    print(company_name)\n",
    "    tweets += api.user_timeline(company_name,count=200,page=1,include_rts=False,include_entities=True,tweet_mode='extended')\n",
    "    for i in range(2,20):\n",
    "        tweets += api.user_timeline(company_name, count=200, page=i,include_rts=False,tweet_mode='extended')\n",
    "    time.sleep(10)\n",
    "tweet_text = []\n",
    "published_date = []\n",
    "Company = []\n",
    "URL = []\n",
    "for tweet in tweets:\n",
    "    if tweet.in_reply_to_status_id == None:\n",
    "        tweet_text.append(tweet.full_text)\n",
    "        published_date.append(tweet.created_at)\n",
    "        Company.append(tweet.user.name)\n",
    "        URL.append(f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\")\n",
    "dat = pd.DataFrame(data=[text for text in tweet_text],columns=['tweets'])\n",
    "dat[\"date\"]=np.array([date for date in published_date])\n",
    "dat[\"Company\"]=np.array([company for company in Company])\n",
    "dat['url']=np.array([url for url in URL])\n",
    "dat.to_csv(\"Raw tweets_December22_10:27PM_CST.csv\",mode='a',index=False)\n",
    "\n",
    "\n",
    "\n",
    "'''---------------------------------------------------------------------------------------\n",
    "Text cleaning:remove @user, URLs, hashtags, non-alphabets characters and special characters (&amp)\n",
    "保留lowercase，轉換為root form，去除長度小於4的單詞'''\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import re\n",
    "stemmer = SnowballStemmer('english')\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "import pandas as pd\n",
    "text = pd.read_csv('Raw tweets_December22_10:27PM_CST.csv')\n",
    "b = []\n",
    "for i,u in text.iterrows():\n",
    "    a = []\n",
    "    word =''\n",
    "    for words in str(u['tweets']).split():\n",
    "        if '@' not in words: #remove @users\n",
    "            #words = words.replace('#','') #remove hashtag symbol\n",
    "            if '#' not in words:\n",
    "                if 'http' not in words: #remove URLs\n",
    "                    if'&amp' not in words: #remove symbol\n",
    "                        words = words.lower()# lower form\n",
    "                        words = re.sub(r'[^a-zA-Z]', ' ', words) #replace non-alphabets characters with space. From \"can't\" to \"can t\"\n",
    "                        if len(words)>3:\n",
    "                            word += (words+' ')\n",
    "    doc = ''\n",
    "    for token in word.split():\n",
    "        if token not in stop_words:\n",
    "            token = porter.stem(token) #root form\n",
    "            doc += (token+' ')\n",
    "    b.append(doc)\n",
    "text['processed']=[i for i in b]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''---------------------------------------------------------------------------------------\n",
    "生成bigram：基於gensim.model.phrase的方式'''\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence),deacc=True)) #將句子變為token，生成list\n",
    "data_words = list(sent_to_words(text['processed'])) #tokenization\n",
    "bigram = gensim.models.Phrases(data_words,min_count=1,threshold=1)\n",
    "'''mincount：兩個單詞共同出現次數小於該值，則不會被考慮為bigram，\n",
    "threshold：Phrases功能中會生成一個'phase score'，超過這個score的bigram會生成在最終結果中，總的來說，mincount越小，threshold越小，最終生成的bigram越多'''\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram) #生成bigram\n",
    "def make_bigrams(texts): #生成bigram\n",
    "    return [bigram[doc] for doc in texts]\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "texts = data_words_bigrams\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "\n",
    "'''---------------------------------------------------------------------------------------\n",
    "counting coherence score,使用的是u_mass'''\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start,limit,step):\n",
    "        model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics,random_state=3)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        i =1\n",
    "        print(i)\n",
    "    return model_list, coherence_values\n",
    "limit=21; start=10; step=1\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=start, limit=limit, step=step)\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values,label='All tweets')\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(range(start,limit,step))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''---------------------------------------------------------------------------------------\n",
    "print bigrams within each topic'''\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=11,random_state=3,alpha='auto',per_word_topics=True)\n",
    "#pprint(lda_model.print_topics(num_words=400))\n",
    "for idx, topic in lda_model.show_topics(formatted=False,num_topics=11,num_words= 400):\n",
    "    print('\\n',end='')\n",
    "    print('Topic:',idx)\n",
    "    for w in topic:\n",
    "        if '_' in w[0]:\n",
    "            print(round(w[1],4),'*',w[0],', ',end='',sep='')\n",
    "\n",
    "\n",
    "'''---------------------------------------------------------------------------------------\n",
    "output LDA，生成一個包含dominant topic，keywords，probability of the topic的csv文件\n",
    "並通過這個文件，統計各個主題內tweets的數量'''\n",
    "\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=text):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: x[1], reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=text['tweets'])\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.to_csv('dominant topics_December_22_10:27PM_CST_data.csv')\n",
    "data = pd.read_csv('dominant topics_December_22_10:27PM_CST_data.csv')\n",
    "# tweets count\n",
    "print('\\n')\n",
    "print('Number of Tweets within each topic:',data.groupby('Dominant_Topic')['Text'].count())\n",
    "\n",
    "'''---------------------------------------------------------------------------------------\n",
    "統計Top 20 'hashtags，通過date parameter，以及drop.cuplicates()，除去重複的'hashtag，\n",
    "也就是說，如果同一個'hashtag在一篇tweet中出現兩次或以上，則只被計算一次'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "data= pd.read_csv('Raw tweets_December22_10:27PM_CST.csv')\n",
    "hashtag = []\n",
    "date = []\n",
    "for index,i in data.iterrows():\n",
    "    for word in i['tweets'].split():\n",
    "        if word.startswith('#'):\n",
    "            date.append(i['date'])\n",
    "            hashtag.append(word)\n",
    "hashtag_trend = pd.DataFrame(data=hashtag,columns=['#'])\n",
    "hashtag_trend['date'] = np.array(date)\n",
    "data.drop_duplicates()\n",
    "counts = collections.Counter(hashtag)\n",
    "print('\\n')\n",
    "print('Top 20 hasgtags:',counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
