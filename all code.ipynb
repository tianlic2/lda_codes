{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load all code.py\n",
    "import pyLDAvis.gensim\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "from gensim.models import CoherenceModel\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import pyLDAvis.gensim\n",
    "import re\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "consumer_key = 'YRWB6yizS63ZJUW2CpEUqS7TN'\n",
    "consumer_secret = 'ZdMYuSvrCPq2qv5yQ01wpE6FgaQJwvytuYuNnkuWdoMLpjO0sg'\n",
    "access_token = '1315688543784562688-UYAqRnekf0mDKoQgsZWoKiBGqlffNw'\n",
    "access_token_secret = 'wvORx0dHxp9uGXXjAjZEJ1hj8eLGcckRlOlmzvxhSqAEA'\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "tweets = []\n",
    "company_twitter_name = ['MemphisMeats','mosa_meat','_superMeat_','ModernMeadow',\n",
    "                        'ethicameat','MissionBarns','BalleticFoods','CubiqF',\n",
    "                        'NewAgeMeats','FinlessFoods','wildtypefoods','itsmeatable',\n",
    "                        'eatGOURMEY','eatjust','shiokmeats','makingmeat','BlueNaluInc','FutureMeat1','AvantMeats',\n",
    "                        'AlephFarms','biftekco','CarnivoresCreed','vowfood','LabFarmFoods']\n",
    "for company_name in company_twitter_name:\n",
    "    print(company_name)\n",
    "    tweets += api.user_timeline(company_name,count=200,page=1,include_rts=False,tweet_mode='extended')\n",
    "    for i in range(2,20):\n",
    "        tweets += api.user_timeline(company_name, count=200, page=i,include_rts=False,tweet_mode='extended')\n",
    "    time.sleep(10)\n",
    "dat = pd.DataFrame(data=[tweet.full_text for tweet in tweets],columns=['tweets'])\n",
    "dat[\"date\"]=np.array([tweet.created_at for tweet in tweets])\n",
    "dat[\"Company\"]=np.array([tweet.user.name for tweet in tweets])\n",
    "dat.to_csv(\"Raw tweets_December_8:37AM.csv\",mode='a',index=False)\n",
    "\n",
    "data = pd.read_csv('Raw tweets_December_8:37AM.csv')\n",
    "stemmer = SnowballStemmer('english')\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# text-cleaning\n",
    "b = []\n",
    "for i,u in data.iterrows():\n",
    "    a = []\n",
    "    word =''\n",
    "    for words in str(u['tweets']).split():\n",
    "        if '@' not in words:\n",
    "            if '#' not in words:\n",
    "                if 'http' not in words:\n",
    "                    if'&amp' not in words:\n",
    "                    # words = re.sub(r'[^a-zA-Z]|(\\w+:\\/\\/\\S+)', ' ',words) #remove non-alphabets characters\n",
    "                        words = re.sub(r'[^a-zA-Z]', '', words) #replace non-alphabets characters with space. From \"can't\" to \"can t\"\n",
    "                        #words = words.replace(' ', '')  # remove space between contraction: From \"can t\" to \"cant\"\n",
    "                        if len(words)>2:\n",
    "                            if words.lower() not in stop_words:\n",
    "                                    words = porter.stem(words)\n",
    "                                    if len(words)>3:\n",
    "                                        words = words.lower()\n",
    "                                        word += (words+' ')\n",
    "    b.append(word)\n",
    "data['processed']=[i for i in b]\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence),deacc=True)) #將句子變為token，生成list\n",
    "data_words = list(sent_to_words(data['processed'])) #tokenization\n",
    "bigram = gensim.models.Phrases(data_words,min_count=1,threshold=1)\n",
    "\n",
    "'''mincount：兩個單詞共同出現次數小於該值，則不會被考慮為bigram，\n",
    "threshold：Phrases功能中會生成一個'phase score'，超過這個score的bigram會生成在最終結果中，總的來說，mincount越小，threshold越小，最終生成的bigram越多'''\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram) #生成bigram\n",
    "trigram = gensim.models.Phrases(bigram[data_words],threshold=1)\n",
    "'''將已生成的bigram再進行一次分析，就會生成將三個（1bigram+single word）\n",
    "或四個單詞（兩個bigram）看為一個整體'''\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "def make_trigram(texts): #對texts進行分析，生成trigram\n",
    "    return [trigram[doc] for doc in texts]\n",
    "def make_bigrams(texts): #生成bigram\n",
    "    return [bigram[doc] for doc in texts]\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "data_words_trigrams = make_trigram(data_words)\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "texts = data_words_bigrams\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(id2word)\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=12,random_state=10,alpha='auto',per_word_topics=True)\n",
    "pprint(lda_model.print_topics(num_words=400))\n",
    "\n",
    "# counting coherence score\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start,limit,step):\n",
    "        model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        i =1\n",
    "        print(i)\n",
    "\n",
    "\n",
    "    return model_list, coherence_values\n",
    "limit=21; start=11; step=1\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words_bigrams, start=start, limit=limit, step=step)\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values,label='All tweets')\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(range(start,limit,step))\n",
    "plt.show()\n",
    "\n",
    "#visualization\n",
    "vis = pyLDAvis.gensim.prepare(lda_model,corpus,id2word)\n",
    "pyLDAvis.save_html(vis,'LDA_Visualization.html')\n",
    "\n",
    "#output LDA\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: x[1], reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data['tweets'])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.to_csv('dominant topics1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
