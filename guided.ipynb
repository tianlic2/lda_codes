{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bd2d8060c6bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from nltk import SnowballStemmer\n",
    "from gensim.models import CoherenceModel\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "#import pyLDAvis.gensim\n",
    "import re\n",
    "stemmer = SnowballStemmer('english')\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "text = pd.read_csv('Raw tweets_December_8:37AM.csv')\n",
    "\n",
    "# text-cleaning\n",
    "b = []\n",
    "for i,u in text.iterrows():\n",
    "    a = []\n",
    "    word =''\n",
    "    for words in str(u['tweets']).split():\n",
    "        if '@' not in words:\n",
    "            if '#' not in words:\n",
    "                if 'http' not in words:\n",
    "                    if'&amp' not in words:\n",
    "                    # words = re.sub(r'[^a-zA-Z]|(\\w+:\\/\\/\\S+)', ' ',words) #remove non-alphabets characters\n",
    "                        words = re.sub(r'[^a-zA-Z]', '', words) #replace non-alphabets characters with space. From \"can't\" to \"can t\"\n",
    "                        #words = words.replace(' ', '')  # remove space between contraction: From \"can t\" to \"cant\"\n",
    "                        if len(words)>2:\n",
    "                            if words.lower() not in stop_words:\n",
    "                                    words = porter.stem(words)\n",
    "                                    if len(words)>3:\n",
    "                                        words = words.lower()\n",
    "                                        word += (words+' ')\n",
    "    b.append(word)\n",
    "text['processed']=[i for i in b]\n",
    "\n",
    "\n",
    "#unigram\n",
    "unigram = []\n",
    "unigram_list = []\n",
    "for index, i in text.iterrows():\n",
    "    unigram=''\n",
    "    for word in i['processed'].split():\n",
    "        unigram+= word+' '\n",
    "    unigram_list.append(unigram)\n",
    "\n",
    "\n",
    "import guidedlda\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "data = pd.read_csv('LDA_U_Processed.csv')\n",
    "\n",
    "model = guidedlda.GuidedLDA(n_topics=7,n_iter=50,random_state=None,refresh=20)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data['processed'])\n",
    "\n",
    "\n",
    "seed_topic_list = [['climat','environmentfriendli','environ','impact','chang','environmentfriendli','environmentallyfriendli'],\n",
    "                   ['cell','cellular','grown','cultur','cellbas','labgrown','cellbas','cellbasedcultur','cellcultiv','cellcultur'],\n",
    "                   ['without','harm','anim','save','help','slaughter','animal','live'],\n",
    "                   ['media','twittermediainternet','multimedia','check','video','blog','blogpost','youtub'],\n",
    "                   ['seafood','meatseafood','seafoodi','fish','fishi','unshellfish','jellyfish'],\n",
    "                   ['look','forward','hire','scientist','join','team'],\n",
    "                   ['futur','first','revolutionari','revolution','revolut']]\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)\n",
    "word2id = dict((v,idx) for idx,v in enumerate(vocab))\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id\n",
    "\n",
    "model.fit(X.toarray(),seed_topics=seed_topics,seed_confidence=0.15)\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 20\n",
    "vocab = tuple(vocab)\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
